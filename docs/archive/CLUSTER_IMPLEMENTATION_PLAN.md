# AiDb å¼¹æ€§é›†ç¾¤å®æ–½è®¡åˆ’

## ğŸ¯ æ¶æ„ç¡®è®¤

### æ ¸å¿ƒæ¶æ„

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Coordinator        â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚ Router           â”‚ â”‚
                    â”‚ â”‚ Load Balancer    â”‚ â”‚
                    â”‚ â”‚ Health Checker   â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚                â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
         â”‚ Shard 1  â”‚     â”‚ Shard 2  â”‚    â”‚ Shard N  â”‚
         â”‚  Group   â”‚     â”‚  Group   â”‚    â”‚  Group   â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚                â”‚
    â”‚                   â”‚      â”‚                â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  ...             ...
â”‚Primary â”‚         â”‚Replica â”‚
â”‚        â”‚         â”‚        â”‚
â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚         â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚
â”‚â”‚Local â”‚â”‚  RPC    â”‚â”‚Cache â”‚â”‚
â”‚â”‚ SSD  â”‚â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚(LRU) â”‚â”‚
â”‚â”‚      â”‚â”‚         â”‚â”‚      â”‚â”‚
â”‚â”‚LSM-  â”‚â”‚         â”‚â””â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚â”‚Tree  â”‚â”‚         â”‚        â”‚
â”‚â””â”€â”€â”¬â”€â”€â”€â”˜â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   â”‚    â”‚
â”‚   â”‚å¼‚æ­¥ â”‚
â”‚   â–¼    â”‚
â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚
â”‚â”‚Backupâ”‚â”‚
â”‚â”‚ Mgr  â”‚â”‚
â”‚â””â”€â”€â”¬â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”¼â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Network   â”‚
â”‚  Storage   â”‚
â”‚ (S3/OSS)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ç»„ä»¶

**1. Coordinatorï¼ˆåè°ƒå™¨ï¼‰**
- è·¯ç”±ï¼šä¸€è‡´æ€§å“ˆå¸Œ
- è´Ÿè½½å‡è¡¡ï¼šè¯»è¯·æ±‚åˆ†å‘
- å¥åº·æ£€æŸ¥ï¼šèŠ‚ç‚¹çŠ¶æ€ç›‘æ§

**2. Shard Groupï¼ˆåˆ†ç‰‡ç»„ï¼‰**
- Primaryï¼šå®Œæ•´å­˜å‚¨ + RPCæœåŠ¡
- Replicasï¼šç¼“å­˜ + è½¬å‘

**3. Primary Nodeï¼ˆä¸»èŠ‚ç‚¹ï¼‰**
- LSMå­˜å‚¨å¼•æ“ï¼ˆæˆ‘ä»¬çš„AiDbï¼‰
- RPCæœåŠ¡ç«¯
- å¤‡ä»½ç®¡ç†å™¨

**4. Replica Nodeï¼ˆä»èŠ‚ç‚¹ï¼‰**
- LRUç¼“å­˜
- RPCå®¢æˆ·ç«¯
- æ— çŠ¶æ€è®¾è®¡

---

## ğŸ“… åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

### æ€»è§ˆ

```
é˜¶æ®µ0: å•æœºç‰ˆ (Week 1-20) âœ… å·²è§„åˆ’
é˜¶æ®µ1: RPCå’Œç½‘ç»œå±‚ (Week 21-24)
é˜¶æ®µ2: Coordinator (Week 25-28)
é˜¶æ®µ3: Shard Group (Week 29-34)
é˜¶æ®µ4: Backupå’Œæ¢å¤ (Week 35-40)
é˜¶æ®µ5: å¼¹æ€§ä¼¸ç¼© (Week 41-44)
é˜¶æ®µ6: ç›‘æ§å’Œè¿ç»´ (Week 45-48)
```

---

## ğŸ“‹ é˜¶æ®µ0: å•æœºç‰ˆï¼ˆå·²è§„åˆ’ï¼ŒWeek 1-20ï¼‰

**ç›®æ ‡**ï¼šå®Œæˆå•æœºLSMå­˜å‚¨å¼•æ“

**å‚è€ƒ**ï¼š`OPTIMIZED_PLAN.md` é˜¶æ®µA/B/C

**äº¤ä»˜ç‰©**ï¼š
- âœ… å®Œæ•´çš„LSM-Treeå®ç°
- âœ… WAL + MemTable + SSTable
- âœ… Compaction + Bloom Filter
- âœ… æ€§èƒ½è¾¾æ ‡ï¼ˆ60-70% RocksDBï¼‰

**éªŒæ”¶æ ‡å‡†**ï¼š
```rust
// èƒ½ç¨³å®šè¿è¡Œ
let db = DB::open("./data", Options::default())?;
for i in 0..1_000_000 {
    db.put(&format!("key{}", i).as_bytes(), b"value")?;
}
// æ€§èƒ½ã€ç¨³å®šæ€§æµ‹è¯•é€šè¿‡
```

---

## ğŸ“‹ é˜¶æ®µ1: RPCå’Œç½‘ç»œå±‚ï¼ˆWeek 21-24ï¼‰

### Week 21: RPCæ¡†æ¶æ­å»º

**ç›®æ ‡**ï¼šå»ºç«‹RPCé€šä¿¡åŸºç¡€

**æŠ€æœ¯é€‰å‹**ï¼š
```toml
[dependencies]
# æ¨è: tonic (gRPC for Rust)
tonic = "0.10"
prost = "0.12"
tokio = { version = "1", features = ["full"] }

# æˆ–è€…: tarpc (çº¯Rust RPC)
# tarpc = "0.33"
```

**å®šä¹‰æœåŠ¡æ¥å£**ï¼š

```protobuf
// proto/aidb.proto
syntax = "proto3";

package aidb;

service Storage {
  // åŸºç¡€æ“ä½œ
  rpc Get(GetRequest) returns (GetResponse);
  rpc Put(PutRequest) returns (PutResponse);
  rpc Delete(DeleteRequest) returns (DeleteResponse);
  
  // æ‰¹é‡æ“ä½œ
  rpc BatchGet(BatchGetRequest) returns (BatchGetResponse);
  rpc BatchPut(BatchPutRequest) returns (BatchPutResponse);
  
  // èŒƒå›´æŸ¥è¯¢
  rpc Scan(ScanRequest) returns (stream ScanResponse);
  
  // å¥åº·æ£€æŸ¥
  rpc HealthCheck(HealthCheckRequest) returns (HealthCheckResponse);
  
  // ç»Ÿè®¡ä¿¡æ¯
  rpc GetStats(GetStatsRequest) returns (GetStatsResponse);
}

message GetRequest {
  bytes key = 1;
}

message GetResponse {
  bool found = 1;
  bytes value = 2;
}

message PutRequest {
  bytes key = 1;
  bytes value = 2;
}

message PutResponse {
  bool success = 1;
}

// ... å…¶ä»–æ¶ˆæ¯å®šä¹‰
```

**å®ç°RPCæœåŠ¡ç«¯**ï¼š

```rust
// src/rpc/server.rs
use tonic::{transport::Server, Request, Response, Status};

pub struct StorageService {
    db: Arc<DB>,
}

#[tonic::async_trait]
impl Storage for StorageService {
    async fn get(&self, request: Request<GetRequest>) 
        -> Result<Response<GetResponse>, Status> {
        let key = &request.get_ref().key;
        
        match self.db.get(key) {
            Ok(Some(value)) => Ok(Response::new(GetResponse {
                found: true,
                value,
            })),
            Ok(None) => Ok(Response::new(GetResponse {
                found: false,
                value: vec![],
            })),
            Err(e) => Err(Status::internal(e.to_string())),
        }
    }
    
    async fn put(&self, request: Request<PutRequest>) 
        -> Result<Response<PutResponse>, Status> {
        let req = request.get_ref();
        
        match self.db.put(&req.key, &req.value) {
            Ok(_) => Ok(Response::new(PutResponse { success: true })),
            Err(e) => Err(Status::internal(e.to_string())),
        }
    }
    
    // å®ç°å…¶ä»–æ–¹æ³•...
}

pub async fn start_server(db: Arc<DB>, addr: SocketAddr) -> Result<()> {
    let service = StorageService { db };
    
    Server::builder()
        .add_service(StorageServer::new(service))
        .serve(addr)
        .await?;
    
    Ok(())
}
```

**å®ç°RPCå®¢æˆ·ç«¯**ï¼š

```rust
// src/rpc/client.rs
use tonic::transport::Channel;

pub struct StorageClient {
    inner: StorageClient<Channel>,
}

impl StorageClient {
    pub async fn connect(addr: &str) -> Result<Self> {
        let inner = StorageClient::connect(addr).await?;
        Ok(Self { inner })
    }
    
    pub async fn get(&mut self, key: &[u8]) -> Result<Option<Vec<u8>>> {
        let request = GetRequest {
            key: key.to_vec(),
        };
        
        let response = self.inner.get(request).await?;
        let reply = response.into_inner();
        
        if reply.found {
            Ok(Some(reply.value))
        } else {
            Ok(None)
        }
    }
    
    pub async fn put(&mut self, key: &[u8], value: &[u8]) -> Result<()> {
        let request = PutRequest {
            key: key.to_vec(),
            value: value.to_vec(),
        };
        
        let response = self.inner.put(request).await?;
        let reply = response.into_inner();
        
        if reply.success {
            Ok(())
        } else {
            Err(Error::internal("put failed"))
        }
    }
    
    // å…¶ä»–æ–¹æ³•...
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] å®šä¹‰å®Œæ•´çš„protobufæ¥å£
- [ ] å®ç°RPCæœåŠ¡ç«¯
- [ ] å®ç°RPCå®¢æˆ·ç«¯
- [ ] è¿æ¥æ± ç®¡ç†
- [ ] é”™è¯¯å¤„ç†å’Œé‡è¯•
- [ ] è¶…æ—¶æ§åˆ¶
- [ ] å•å…ƒæµ‹è¯•

---

### Week 22: PrimaryèŠ‚ç‚¹å®ç°

**ç›®æ ‡**ï¼šåŒ…è£…DBä¸ºPrimaryèŠ‚ç‚¹ï¼Œæä¾›RPCæœåŠ¡

```rust
// src/cluster/primary.rs
pub struct PrimaryNode {
    // æœ¬åœ°DBå®ä¾‹
    db: Arc<DB>,
    
    // RPCæœåŠ¡å™¨
    rpc_server: RpcServer,
    
    // é…ç½®
    config: PrimaryConfig,
    
    // ç»Ÿè®¡ä¿¡æ¯
    stats: Arc<RwLock<PrimaryStats>>,
}

pub struct PrimaryConfig {
    // ç›‘å¬åœ°å€
    listen_addr: SocketAddr,
    
    // DBè·¯å¾„
    db_path: PathBuf,
    
    // DBé…ç½®
    db_options: Options,
    
    // RPCé…ç½®
    max_connections: usize,
    request_timeout: Duration,
}

impl PrimaryNode {
    pub async fn new(config: PrimaryConfig) -> Result<Self> {
        // 1. æ‰“å¼€æœ¬åœ°DB
        let db = Arc::new(DB::open(&config.db_path, config.db_options)?);
        
        // 2. åˆ›å»ºRPCæœåŠ¡å™¨
        let rpc_server = RpcServer::new(db.clone(), config.listen_addr);
        
        // 3. åˆå§‹åŒ–ç»Ÿè®¡
        let stats = Arc::new(RwLock::new(PrimaryStats::default()));
        
        Ok(Self {
            db,
            rpc_server,
            config,
            stats,
        })
    }
    
    pub async fn start(&mut self) -> Result<()> {
        info!("Starting primary node at {}", self.config.listen_addr);
        
        // å¯åŠ¨RPCæœåŠ¡å™¨
        self.rpc_server.start().await?;
        
        Ok(())
    }
    
    pub async fn stop(&mut self) -> Result<()> {
        info!("Stopping primary node");
        
        // åœæ­¢RPCæœåŠ¡å™¨
        self.rpc_server.stop().await?;
        
        // å…³é—­DB
        self.db.close()?;
        
        Ok(())
    }
    
    // æœ¬åœ°å†™å…¥ï¼ˆä¹Ÿå¯é€šè¿‡RPCï¼‰
    pub async fn put(&self, key: &[u8], value: &[u8]) -> Result<()> {
        let start = Instant::now();
        
        self.db.put(key, value)?;
        
        // æ›´æ–°ç»Ÿè®¡
        self.stats.write().record_put(start.elapsed());
        
        Ok(())
    }
    
    // æœ¬åœ°è¯»å–
    pub async fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {
        let start = Instant::now();
        
        let result = self.db.get(key)?;
        
        // æ›´æ–°ç»Ÿè®¡
        self.stats.write().record_get(start.elapsed(), result.is_some());
        
        Ok(result)
    }
}

#[derive(Default)]
pub struct PrimaryStats {
    pub total_gets: u64,
    pub total_puts: u64,
    pub get_latency: LatencyStats,
    pub put_latency: LatencyStats,
    // ... æ›´å¤šç»Ÿè®¡
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] å®ç°PrimaryNodeç»“æ„
- [ ] é›†æˆRPCæœåŠ¡å™¨
- [ ] ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
- [ ] å¥åº·æ£€æŸ¥ç«¯ç‚¹
- [ ] ä¼˜é›…å…³é—­
- [ ] é›†æˆæµ‹è¯•

---

### Week 23: ReplicaèŠ‚ç‚¹å®ç°

**ç›®æ ‡**ï¼šå®ç°è½»é‡çº§ç¼“å­˜èŠ‚ç‚¹

```rust
// src/cluster/replica.rs
use lru::LruCache;

pub struct ReplicaNode {
    // LRUç¼“å­˜
    cache: Arc<RwLock<LruCache<Vec<u8>, CachedValue>>>,
    
    // Primaryçš„RPCå®¢æˆ·ç«¯
    primary_client: Arc<Mutex<StorageClient>>,
    
    // é…ç½®
    config: ReplicaConfig,
    
    // ç»Ÿè®¡
    stats: Arc<RwLock<ReplicaStats>>,
}

pub struct ReplicaConfig {
    // Primaryåœ°å€
    primary_addr: String,
    
    // ç¼“å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    cache_size: usize,
    
    // é¢„çƒ­ç­–ç•¥
    warmup_strategy: WarmupStrategy,
    
    // ç¼“å­˜å¤±æ•ˆç­–ç•¥
    invalidation_policy: InvalidationPolicy,
}

#[derive(Clone)]
pub struct CachedValue {
    value: Vec<u8>,
    cached_at: Instant,
    access_count: u64,
}

impl ReplicaNode {
    pub async fn new(config: ReplicaConfig) -> Result<Self> {
        // 1. è¿æ¥Primary
        let primary_client = Arc::new(Mutex::new(
            StorageClient::connect(&config.primary_addr).await?
        ));
        
        // 2. åˆ›å»ºç¼“å­˜
        let cache_capacity = config.cache_size / 1024; // å‡è®¾å¹³å‡1KB/entry
        let cache = Arc::new(RwLock::new(
            LruCache::new(cache_capacity.try_into().unwrap())
        ));
        
        // 3. åˆå§‹åŒ–ç»Ÿè®¡
        let stats = Arc::new(RwLock::new(ReplicaStats::default()));
        
        let node = Self {
            cache,
            primary_client,
            config,
            stats,
        };
        
        // 4. é¢„çƒ­ï¼ˆå¦‚æœé…ç½®ï¼‰
        node.warmup().await?;
        
        Ok(node)
    }
    
    // è¯»å–æ“ä½œ
    pub async fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {
        let start = Instant::now();
        
        // 1. æŸ¥ç¼“å­˜
        {
            let mut cache = self.cache.write();
            if let Some(cached) = cache.get_mut(key) {
                // ç¼“å­˜å‘½ä¸­
                cached.access_count += 1;
                self.stats.write().record_cache_hit(start.elapsed());
                return Ok(Some(cached.value.clone()));
            }
        }
        
        // 2. ç¼“å­˜missï¼Œè½¬å‘åˆ°Primary
        let mut client = self.primary_client.lock().await;
        let value = client.get(key).await?;
        
        // 3. æ›´æ–°ç¼“å­˜
        if let Some(ref v) = value {
            let mut cache = self.cache.write();
            cache.put(key.to_vec(), CachedValue {
                value: v.clone(),
                cached_at: Instant::now(),
                access_count: 1,
            });
        }
        
        self.stats.write().record_cache_miss(start.elapsed());
        Ok(value)
    }
    
    // å†™æ“ä½œï¼ˆè½¬å‘åˆ°Primaryï¼‰
    pub async fn put(&self, key: &[u8], value: &[u8]) -> Result<()> {
        // 1. è½¬å‘åˆ°Primary
        let mut client = self.primary_client.lock().await;
        client.put(key, value).await?;
        
        // 2. ä½¿ç¼“å­˜å¤±æ•ˆ
        let mut cache = self.cache.write();
        cache.pop(key);
        
        Ok(())
    }
    
    // é¢„çƒ­
    async fn warmup(&self) -> Result<()> {
        match &self.config.warmup_strategy {
            WarmupStrategy::None => Ok(()),
            
            WarmupStrategy::HotKeys(keys) => {
                info!("Warming up {} hot keys", keys.len());
                let mut client = self.primary_client.lock().await;
                
                for key in keys {
                    if let Some(value) = client.get(key).await? {
                        let mut cache = self.cache.write();
                        cache.put(key.clone(), CachedValue {
                            value,
                            cached_at: Instant::now(),
                            access_count: 0,
                        });
                    }
                }
                
                Ok(())
            }
            
            WarmupStrategy::RangeScan { start, end, limit } => {
                info!("Warming up with range scan");
                // å®ç°èŒƒå›´æ‰«æé¢„çƒ­
                // ...
                Ok(())
            }
        }
    }
    
    // ç¼“å­˜ç»Ÿè®¡
    pub fn cache_stats(&self) -> CacheStats {
        let cache = self.cache.read();
        let stats = self.stats.read();
        
        CacheStats {
            size: cache.len(),
            capacity: cache.cap(),
            hit_rate: stats.hit_rate(),
            // ...
        }
    }
}

#[derive(Default)]
pub struct ReplicaStats {
    pub cache_hits: u64,
    pub cache_misses: u64,
    pub forwarded_gets: u64,
    pub forwarded_puts: u64,
    // ...
}

impl ReplicaStats {
    pub fn hit_rate(&self) -> f64 {
        let total = self.cache_hits + self.cache_misses;
        if total == 0 {
            0.0
        } else {
            self.cache_hits as f64 / total as f64
        }
    }
}

pub enum WarmupStrategy {
    None,
    HotKeys(Vec<Vec<u8>>),
    RangeScan {
        start: Vec<u8>,
        end: Vec<u8>,
        limit: usize,
    },
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] å®ç°ReplicaNodeç»“æ„
- [ ] LRUç¼“å­˜é›†æˆ
- [ ] RPCå®¢æˆ·ç«¯è¿æ¥ç®¡ç†
- [ ] é¢„çƒ­ç­–ç•¥å®ç°
- [ ] ç¼“å­˜å¤±æ•ˆç­–ç•¥
- [ ] ç»Ÿè®¡ä¿¡æ¯
- [ ] å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

---

### Week 24: ç½‘ç»œå±‚ä¼˜åŒ–

**ç›®æ ‡**ï¼šè¿æ¥æ± ã€è¶…æ—¶ã€é‡è¯•ã€ç›‘æ§

```rust
// src/rpc/pool.rs
pub struct ConnectionPool {
    addr: String,
    pool: Vec<StorageClient>,
    max_size: usize,
    idle_timeout: Duration,
}

impl ConnectionPool {
    pub async fn new(addr: String, max_size: usize) -> Result<Self> {
        let mut pool = Vec::with_capacity(max_size);
        
        // é¢„åˆ›å»ºä¸€äº›è¿æ¥
        for _ in 0..max_size.min(4) {
            let client = StorageClient::connect(&addr).await?;
            pool.push(client);
        }
        
        Ok(Self {
            addr,
            pool,
            max_size,
            idle_timeout: Duration::from_secs(60),
        })
    }
    
    pub async fn get_client(&mut self) -> Result<StorageClient> {
        if let Some(client) = self.pool.pop() {
            Ok(client)
        } else if self.pool.len() < self.max_size {
            StorageClient::connect(&self.addr).await
        } else {
            // ç­‰å¾…å¯ç”¨è¿æ¥
            Err(Error::internal("Connection pool exhausted"))
        }
    }
    
    pub fn return_client(&mut self, client: StorageClient) {
        if self.pool.len() < self.max_size {
            self.pool.push(client);
        }
        // else: drop connection
    }
}

// src/rpc/retry.rs
pub struct RetryPolicy {
    max_retries: usize,
    backoff: ExponentialBackoff,
}

impl RetryPolicy {
    pub async fn execute<F, T>(&self, mut f: F) -> Result<T>
    where
        F: FnMut() -> BoxFuture<'static, Result<T>>,
    {
        let mut retries = 0;
        let mut delay = self.backoff.initial_delay;
        
        loop {
            match f().await {
                Ok(result) => return Ok(result),
                Err(e) if retries < self.max_retries && e.is_retryable() => {
                    retries += 1;
                    warn!("Retry {} after error: {}", retries, e);
                    sleep(delay).await;
                    delay *= 2; // æŒ‡æ•°é€€é¿
                }
                Err(e) => return Err(e),
            }
        }
    }
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] è¿æ¥æ± å®ç°
- [ ] è¶…æ—¶æ§åˆ¶
- [ ] é‡è¯•ç­–ç•¥
- [ ] é”™è¯¯åˆ†ç±»ï¼ˆå¯é‡è¯•/ä¸å¯é‡è¯•ï¼‰
- [ ] æŒ‡æ ‡æ”¶é›†ï¼ˆå»¶è¿Ÿã€é”™è¯¯ç‡ï¼‰
- [ ] å‹åŠ›æµ‹è¯•

**é˜¶æ®µ1äº¤ä»˜ç‰©**ï¼š
- âœ… å®Œæ•´çš„RPCæ¡†æ¶
- âœ… PrimaryèŠ‚ç‚¹å¯é€šè¿‡RPCè®¿é—®
- âœ… ReplicaèŠ‚ç‚¹å¯ç¼“å­˜å’Œè½¬å‘
- âœ… æ€§èƒ½æµ‹è¯•é€šè¿‡

---

## ğŸ“‹ é˜¶æ®µ2: Coordinatorï¼ˆWeek 25-28ï¼‰

### Week 25: ä¸€è‡´æ€§å“ˆå¸Œå®ç°

**ç›®æ ‡**ï¼šå®ç°è·¯ç”±åŸºç¡€

```rust
// src/cluster/consistent_hash.rs
use std::collections::BTreeMap;

pub struct ConsistentHashRing {
    // è™šæ‹ŸèŠ‚ç‚¹æ˜ å°„åˆ°å®é™…èŠ‚ç‚¹
    ring: BTreeMap<u64, ShardId>,
    
    // è™šæ‹ŸèŠ‚ç‚¹æ•°é‡ï¼ˆæ¯ä¸ªå®é™…èŠ‚ç‚¹ï¼‰
    virtual_nodes: usize,
    
    // èŠ‚ç‚¹åˆ—è¡¨
    nodes: HashMap<ShardId, NodeInfo>,
}

impl ConsistentHashRing {
    pub fn new(virtual_nodes: usize) -> Self {
        Self {
            ring: BTreeMap::new(),
            virtual_nodes,
            nodes: HashMap::new(),
        }
    }
    
    pub fn add_node(&mut self, shard_id: ShardId, info: NodeInfo) {
        // æ·»åŠ è™šæ‹ŸèŠ‚ç‚¹
        for i in 0..self.virtual_nodes {
            let key = format!("{}-{}", shard_id, i);
            let hash = hash_key(key.as_bytes());
            self.ring.insert(hash, shard_id);
        }
        
        self.nodes.insert(shard_id, info);
    }
    
    pub fn remove_node(&mut self, shard_id: ShardId) {
        // ç§»é™¤è™šæ‹ŸèŠ‚ç‚¹
        for i in 0..self.virtual_nodes {
            let key = format!("{}-{}", shard_id, i);
            let hash = hash_key(key.as_bytes());
            self.ring.remove(&hash);
        }
        
        self.nodes.remove(&shard_id);
    }
    
    pub fn get_node(&self, key: &[u8]) -> Option<ShardId> {
        if self.ring.is_empty() {
            return None;
        }
        
        let hash = hash_key(key);
        
        // æ‰¾åˆ°ç¬¬ä¸€ä¸ª >= hash çš„èŠ‚ç‚¹
        self.ring.range(hash..)
            .next()
            .or_else(|| self.ring.iter().next())
            .map(|(_, shard_id)| *shard_id)
    }
}

fn hash_key(key: &[u8]) -> u64 {
    use std::hash::{Hash, Hasher};
    use std::collections::hash_map::DefaultHasher;
    
    let mut hasher = DefaultHasher::new();
    key.hash(&mut hasher);
    hasher.finish()
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] ä¸€è‡´æ€§å“ˆå¸Œå®ç°
- [ ] è™šæ‹ŸèŠ‚ç‚¹æ”¯æŒ
- [ ] èŠ‚ç‚¹å¢åˆ 
- [ ] è´Ÿè½½å‡è¡¡éªŒè¯
- [ ] å•å…ƒæµ‹è¯•

---

### Week 26: Coordinatoræ ¸å¿ƒé€»è¾‘

**ç›®æ ‡**ï¼šå®ç°è·¯ç”±å’Œåˆ†å‘

```rust
// src/cluster/coordinator.rs
pub struct Coordinator {
    // ä¸€è‡´æ€§å“ˆå¸Œç¯
    hash_ring: Arc<RwLock<ConsistentHashRing>>,
    
    // Shard Groupæ˜ å°„
    shard_groups: Arc<RwLock<HashMap<ShardId, ShardGroup>>>,
    
    // é…ç½®
    config: CoordinatorConfig,
    
    // ç»Ÿè®¡
    stats: Arc<RwLock<CoordinatorStats>>,
}

pub struct ShardGroup {
    pub shard_id: ShardId,
    pub primary: PrimaryInfo,
    pub replicas: Vec<ReplicaInfo>,
    pub health: ShardHealth,
}

pub struct PrimaryInfo {
    pub addr: String,
    pub client: Arc<Mutex<StorageClient>>,
}

pub struct ReplicaInfo {
    pub id: ReplicaId,
    pub addr: String,
    pub client: Arc<Mutex<StorageClient>>,
    pub load: AtomicU64, // å½“å‰è¿æ¥æ•°/è¯·æ±‚æ•°
}

impl Coordinator {
    pub async fn new(config: CoordinatorConfig) -> Result<Self> {
        let hash_ring = Arc::new(RwLock::new(
            ConsistentHashRing::new(config.virtual_nodes)
        ));
        
        let shard_groups = Arc::new(RwLock::new(HashMap::new()));
        
        let stats = Arc::new(RwLock::new(CoordinatorStats::default()));
        
        Ok(Self {
            hash_ring,
            shard_groups,
            config,
            stats,
        })
    }
    
    // æ³¨å†ŒShard
    pub async fn register_shard(&self, shard: ShardGroup) -> Result<()> {
        let shard_id = shard.shard_id;
        
        // 1. æ·»åŠ åˆ°hash ring
        self.hash_ring.write().add_node(shard_id, NodeInfo {
            addr: shard.primary.addr.clone(),
        });
        
        // 2. æ³¨å†Œshard group
        self.shard_groups.write().insert(shard_id, shard);
        
        info!("Registered shard {}", shard_id);
        Ok(())
    }
    
    // è·¯ç”±keyåˆ°å¯¹åº”shard
    fn route_key(&self, key: &[u8]) -> Result<ShardId> {
        let hash_ring = self.hash_ring.read();
        hash_ring.get_node(key)
            .ok_or_else(|| Error::internal("No shard available"))
    }
    
    // å†™æ“ä½œï¼ˆè·¯ç”±åˆ°Primaryï¼‰
    pub async fn put(&self, key: &[u8], value: &[u8]) -> Result<()> {
        let start = Instant::now();
        
        // 1. è·¯ç”±åˆ°shard
        let shard_id = self.route_key(key)?;
        
        // 2. è·å–shard
        let shard_groups = self.shard_groups.read();
        let shard = shard_groups.get(&shard_id)
            .ok_or_else(|| Error::internal("Shard not found"))?;
        
        // 3. å†™å…¥Primary
        let mut client = shard.primary.client.lock().await;
        client.put(key, value).await?;
        
        // 4. ç»Ÿè®¡
        self.stats.write().record_put(start.elapsed());
        
        Ok(())
    }
    
    // è¯»æ“ä½œï¼ˆè´Ÿè½½å‡è¡¡åˆ°Replicaæˆ–Primaryï¼‰
    pub async fn get(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {
        let start = Instant::now();
        
        // 1. è·¯ç”±åˆ°shard
        let shard_id = self.route_key(key)?;
        
        // 2. è·å–shard
        let shard_groups = self.shard_groups.read();
        let shard = shard_groups.get(&shard_id)
            .ok_or_else(|| Error::internal("Shard not found"))?;
        
        // 3. é€‰æ‹©èŠ‚ç‚¹ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
        let node = self.select_read_node(shard)?;
        
        // 4. è¯»å–
        let mut client = node.lock().await;
        let result = client.get(key).await?;
        
        // 5. ç»Ÿè®¡
        self.stats.write().record_get(start.elapsed(), result.is_some());
        
        Ok(result)
    }
    
    // è´Ÿè½½å‡è¡¡é€‰æ‹©è¯»èŠ‚ç‚¹
    fn select_read_node(&self, shard: &ShardGroup) 
        -> Result<Arc<Mutex<StorageClient>>> {
        // ç­–ç•¥1: è½®è¯¢
        // ç­–ç•¥2: æœ€å°‘è¿æ¥
        // ç­–ç•¥3: å“åº”æ—¶é—´åŠ æƒ
        
        match self.config.load_balance_strategy {
            LoadBalanceStrategy::RoundRobin => {
                self.round_robin_select(shard)
            }
            LoadBalanceStrategy::LeastConnections => {
                self.least_connections_select(shard)
            }
            LoadBalanceStrategy::Random => {
                self.random_select(shard)
            }
        }
    }
    
    fn random_select(&self, shard: &ShardGroup) 
        -> Result<Arc<Mutex<StorageClient>>> {
        if shard.replicas.is_empty() {
            // æ²¡æœ‰replicaï¼Œç”¨primary
            return Ok(shard.primary.client.clone());
        }
        
        // éšæœºé€‰æ‹©replica
        let idx = rand::random::<usize>() % (shard.replicas.len() + 1);
        
        if idx == shard.replicas.len() {
            Ok(shard.primary.client.clone())
        } else {
            Ok(shard.replicas[idx].client.clone())
        }
    }
    
    fn least_connections_select(&self, shard: &ShardGroup) 
        -> Result<Arc<Mutex<StorageClient>>> {
        // é€‰æ‹©å½“å‰è´Ÿè½½æœ€ä½çš„èŠ‚ç‚¹
        let mut min_load = u64::MAX;
        let mut selected = None;
        
        // æ£€æŸ¥primary
        // ï¼ˆå‡è®¾primaryä¹Ÿæœ‰loadç»Ÿè®¡ï¼‰
        
        // æ£€æŸ¥replicas
        for replica in &shard.replicas {
            let load = replica.load.load(Ordering::Relaxed);
            if load < min_load {
                min_load = load;
                selected = Some(replica.client.clone());
            }
        }
        
        selected.or(Some(shard.primary.client.clone()))
            .ok_or_else(|| Error::internal("No node available"))
    }
}

pub enum LoadBalanceStrategy {
    RoundRobin,
    LeastConnections,
    Random,
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] Coordinatoræ ¸å¿ƒç»“æ„
- [ ] Shardæ³¨å†Œå’Œç®¡ç†
- [ ] è·¯ç”±å®ç°
- [ ] è´Ÿè½½å‡è¡¡ç­–ç•¥
- [ ] ç»Ÿè®¡æ”¶é›†
- [ ] é›†æˆæµ‹è¯•

---

### Week 27-28: å¥åº·æ£€æŸ¥å’Œæ•…éšœå¤„ç†

**ç›®æ ‡**ï¼šç›‘æ§èŠ‚ç‚¹å¥åº·ï¼Œå¤„ç†æ•…éšœ

```rust
// src/cluster/health.rs
pub struct HealthChecker {
    coordinator: Arc<Coordinator>,
    check_interval: Duration,
    timeout: Duration,
}

impl HealthChecker {
    pub async fn start(&self) {
        let mut interval = tokio::time::interval(self.check_interval);
        
        loop {
            interval.tick().await;
            self.check_all_shards().await;
        }
    }
    
    async fn check_all_shards(&self) {
        let shard_groups = self.coordinator.shard_groups.read().clone();
        
        for (shard_id, shard) in shard_groups {
            // æ£€æŸ¥Primary
            if !self.check_primary(&shard).await {
                warn!("Primary of shard {} is unhealthy", shard_id);
                self.handle_primary_failure(shard_id).await;
            }
            
            // æ£€æŸ¥Replicas
            for replica in &shard.replicas {
                if !self.check_replica(replica).await {
                    warn!("Replica {} of shard {} is unhealthy", 
                          replica.id, shard_id);
                    self.handle_replica_failure(shard_id, replica.id).await;
                }
            }
        }
    }
    
    async fn check_primary(&self, shard: &ShardGroup) -> bool {
        let mut client = shard.primary.client.lock().await;
        
        match timeout(self.timeout, client.health_check()).await {
            Ok(Ok(_)) => true,
            _ => false,
        }
    }
    
    async fn handle_primary_failure(&self, shard_id: ShardId) {
        // 1. æ ‡è®°ä¸ºä¸å¥åº·
        // 2. åœæ­¢è·¯ç”±å†™å…¥åˆ°æ­¤shard
        // 3. è§¦å‘å‘Šè­¦
        // 4. ç­‰å¾…Primaryæ¢å¤æˆ–æ‰‹åŠ¨å¹²é¢„
        
        error!("Primary failure for shard {}, stopping writes", shard_id);
        
        // å¯é€‰ï¼šè‡ªåŠ¨ä»å¤‡ä»½æ¢å¤ï¼ˆåç»­å®ç°ï¼‰
    }
    
    async fn handle_replica_failure(&self, shard_id: ShardId, replica_id: ReplicaId) {
        // ä»å¯ç”¨åˆ—è¡¨ä¸­ç§»é™¤
        let mut shard_groups = self.coordinator.shard_groups.write();
        if let Some(shard) = shard_groups.get_mut(&shard_id) {
            shard.replicas.retain(|r| r.id != replica_id);
        }
        
        warn!("Removed failed replica {} from shard {}", replica_id, shard_id);
    }
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] å¥åº·æ£€æŸ¥å®ç°
- [ ] å®šæœŸæ£€æµ‹
- [ ] æ•…éšœå¤„ç†
- [ ] å‘Šè­¦é›†æˆ
- [ ] è‡ªåŠ¨æ¢å¤ï¼ˆå¯é€‰ï¼‰
- [ ] æ•…éšœæ³¨å…¥æµ‹è¯•

**é˜¶æ®µ2äº¤ä»˜ç‰©**ï¼š
- âœ… Coordinatorå¯ä»¥è·¯ç”±è¯·æ±‚
- âœ… è´Ÿè½½å‡è¡¡å·¥ä½œæ­£å¸¸
- âœ… å¥åº·æ£€æŸ¥å’Œæ•…éšœå¤„ç†
- âœ… å¤šshardæµ‹è¯•é€šè¿‡

---

## ğŸ“‹ é˜¶æ®µ3: Shard Groupï¼ˆWeek 29-34ï¼‰

### Week 29-30: å®Œæ•´çš„Shard Groupå®ç°

**ç›®æ ‡**ï¼šæ•´åˆPrimaryå’ŒReplicaï¼Œå½¢æˆå®Œæ•´çš„Shard

```rust
// src/cluster/shard_group.rs
pub struct ShardGroupManager {
    config: ShardGroupConfig,
    primary: Option<PrimaryNode>,
    replicas: Vec<ReplicaNode>,
    state: ShardState,
}

pub struct ShardGroupConfig {
    pub shard_id: ShardId,
    pub data_dir: PathBuf,
    pub primary_config: PrimaryConfig,
    pub replica_configs: Vec<ReplicaConfig>,
}

#[derive(Debug, Clone)]
pub enum ShardState {
    Initializing,
    Running,
    Degraded,  // Primaryæˆ–éƒ¨åˆ†Replicaä¸å¯ç”¨
    Stopped,
}

impl ShardGroupManager {
    pub async fn new(config: ShardGroupConfig) -> Result<Self> {
        Ok(Self {
            config,
            primary: None,
            replicas: Vec::new(),
            state: ShardState::Initializing,
        })
    }
    
    pub async fn start(&mut self) -> Result<()> {
        info!("Starting shard group {}", self.config.shard_id);
        
        // 1. å¯åŠ¨Primary
        let mut primary = PrimaryNode::new(self.config.primary_config.clone()).await?;
        primary.start().await?;
        self.primary = Some(primary);
        
        // 2. å¯åŠ¨Replicas
        for replica_config in &self.config.replica_configs {
            let replica = ReplicaNode::new(replica_config.clone()).await?;
            self.replicas.push(replica);
        }
        
        self.state = ShardState::Running;
        info!("Shard group {} is running", self.config.shard_id);
        
        Ok(())
    }
    
    pub async fn stop(&mut self) -> Result<()> {
        info!("Stopping shard group {}", self.config.shard_id);
        
        // 1. åœæ­¢Replicas
        self.replicas.clear();
        
        // 2. åœæ­¢Primary
        if let Some(mut primary) = self.primary.take() {
            primary.stop().await?;
        }
        
        self.state = ShardState::Stopped;
        Ok(())
    }
    
    // æ·»åŠ Replica
    pub async fn add_replica(&mut self, config: ReplicaConfig) -> Result<()> {
        let replica = ReplicaNode::new(config).await?;
        self.replicas.push(replica);
        
        info!("Added replica to shard {}", self.config.shard_id);
        Ok(())
    }
    
    // ç§»é™¤Replica
    pub async fn remove_replica(&mut self, replica_id: ReplicaId) -> Result<()> {
        self.replicas.retain(|r| r.id() != replica_id);
        
        info!("Removed replica {} from shard {}", replica_id, self.config.shard_id);
        Ok(())
    }
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] ShardGroupManagerå®ç°
- [ ] Primaryå’ŒReplicaç”Ÿå‘½å‘¨æœŸç®¡ç†
- [ ] åŠ¨æ€æ·»åŠ /ç§»é™¤Replica
- [ ] çŠ¶æ€ç®¡ç†
- [ ] é›†æˆæµ‹è¯•

---

### Week 31-32: å¤šShardé›†æˆæµ‹è¯•

**ç›®æ ‡**ï¼šéªŒè¯å¤šä¸ªShardåŒæ—¶è¿è¡Œ

```rust
// tests/multi_shard_test.rs
#[tokio::test]
async fn test_multi_shard_cluster() -> Result<()> {
    // 1. å¯åŠ¨Coordinator
    let coordinator = Coordinator::new(CoordinatorConfig::default()).await?;
    
    // 2. åˆ›å»º3ä¸ªShard Groups
    for i in 0..3 {
        let shard_id = ShardId::new(i);
        let primary_addr = format!("127.0.0.1:{}", 9000 + i);
        
        // å¯åŠ¨Primary
        let primary_config = PrimaryConfig {
            listen_addr: primary_addr.parse()?,
            db_path: format!("/tmp/test_shard_{}", i).into(),
            ..Default::default()
        };
        let mut primary = PrimaryNode::new(primary_config).await?;
        primary.start().await?;
        
        // åˆ›å»ºShard Group
        let shard_group = ShardGroup {
            shard_id,
            primary: PrimaryInfo {
                addr: primary_addr.clone(),
                client: Arc::new(Mutex::new(
                    StorageClient::connect(&primary_addr).await?
                )),
            },
            replicas: vec![],
            health: ShardHealth::Healthy,
        };
        
        // æ³¨å†Œåˆ°Coordinator
        coordinator.register_shard(shard_group).await?;
    }
    
    // 3. å†™å…¥æµ‹è¯•æ•°æ®
    for i in 0..1000 {
        let key = format!("key{}", i).into_bytes();
        let value = format!("value{}", i).into_bytes();
        coordinator.put(&key, &value).await?;
    }
    
    // 4. éªŒè¯è¯»å–
    for i in 0..1000 {
        let key = format!("key{}", i).into_bytes();
        let value = coordinator.get(&key).await?;
        assert_eq!(value, Some(format!("value{}", i).into_bytes()));
    }
    
    // 5. éªŒè¯æ•°æ®åˆ†å¸ƒ
    // æ£€æŸ¥æ¯ä¸ªshardçš„æ•°æ®é‡å¤§è‡´å‡è¡¡
    
    Ok(())
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] å¤šshardå¯åŠ¨æµ‹è¯•
- [ ] æ•°æ®åˆ†å¸ƒéªŒè¯
- [ ] è´Ÿè½½å‡è¡¡æµ‹è¯•
- [ ] æ•…éšœåœºæ™¯æµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•

---

### Week 33-34: æ€§èƒ½ä¼˜åŒ–å’Œå‹åŠ›æµ‹è¯•

**ç›®æ ‡**ï¼šè¾¾åˆ°æ€§èƒ½ç›®æ ‡

**æ€§èƒ½æµ‹è¯•**ï¼š
```rust
// benches/cluster_bench.rs
use criterion::{criterion_group, criterion_main, Criterion, Throughput};

fn bench_cluster_write(c: &mut Criterion) {
    let runtime = tokio::runtime::Runtime::new().unwrap();
    let coordinator = runtime.block_on(setup_cluster()).unwrap();
    
    let mut group = c.benchmark_group("cluster_write");
    group.throughput(Throughput::Elements(1));
    
    group.bench_function("put", |b| {
        b.iter(|| {
            runtime.block_on(async {
                let key = format!("key{}", rand::random::<u64>()).into_bytes();
                let value = vec![0u8; 1024]; // 1KB value
                coordinator.put(&key, &value).await.unwrap();
            });
        });
    });
    
    group.finish();
}

fn bench_cluster_read(c: &mut Criterion) {
    // ç±»ä¼¼å®ç°...
}

criterion_group!(benches, bench_cluster_write, bench_cluster_read);
criterion_main!(benches);
```

**æ€§èƒ½ç›®æ ‡**ï¼š

| åœºæ™¯ | ç›®æ ‡ | è¯´æ˜ |
|------|------|------|
| å•shardå†™å…¥ | 50K ops/s | å—å•æœºDBæ€§èƒ½é™åˆ¶ |
| 10 shardså†™å…¥ | 500K ops/s | çº¿æ€§æ‰©å±• |
| Replicaç¼“å­˜å‘½ä¸­è¯» | 500K ops/s | å†…å­˜ç¼“å­˜ |
| Replicaç¼“å­˜missè¯» | 30K ops/s | RPCè½¬å‘å¼€é”€ |

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] ç“¶é¢ˆè¯†åˆ«å’Œä¼˜åŒ–
- [ ] å‹åŠ›æµ‹è¯•ï¼ˆé•¿æ—¶é—´è¿è¡Œï¼‰
- [ ] å†…å­˜æ³„æ¼æ£€æŸ¥
- [ ] æ€§èƒ½æ–‡æ¡£

**é˜¶æ®µ3äº¤ä»˜ç‰©**ï¼š
- âœ… å®Œæ•´çš„Shard Group
- âœ… å¤šshardååŒå·¥ä½œ
- âœ… æ€§èƒ½è¾¾æ ‡
- âœ… ç¨³å®šæ€§éªŒè¯

---

## ğŸ“‹ é˜¶æ®µ4: Backupå’Œæ¢å¤ï¼ˆWeek 35-40ï¼‰

### Week 35-36: å¤‡ä»½ç®¡ç†å™¨

**ç›®æ ‡**ï¼šå®ç°å¼‚æ­¥å¤‡ä»½åˆ°ç½‘ç›˜

```rust
// src/backup/manager.rs
pub struct BackupManager {
    db: Arc<DB>,
    storage: Arc<dyn BackupStorage>,
    config: BackupConfig,
    state: Arc<RwLock<BackupState>>,
}

pub struct BackupConfig {
    // å¿«ç…§é—´éš”
    snapshot_interval: Duration,
    
    // WALå½’æ¡£é—´éš”
    wal_archive_interval: Duration,
    
    // ä¿ç•™ç­–ç•¥
    retention_policy: RetentionPolicy,
    
    // å¹¶å‘åº¦
    concurrent_uploads: usize,
}

pub struct RetentionPolicy {
    // ä¿ç•™æœ€è¿‘Nä¸ªå¿«ç…§
    keep_snapshots: usize,
    
    // ä¿ç•™Nå¤©å†…çš„WAL
    keep_wal_days: u32,
}

// å¤‡ä»½å­˜å‚¨æŠ½è±¡
#[async_trait]
pub trait BackupStorage: Send + Sync {
    async fn upload_file(&self, local_path: &Path, remote_path: &str) -> Result<()>;
    async fn download_file(&self, remote_path: &str, local_path: &Path) -> Result<()>;
    async fn list_files(&self, prefix: &str) -> Result<Vec<String>>;
    async fn delete_file(&self, remote_path: &str) -> Result<()>;
}

// S3/OSSå®ç°
pub struct S3Storage {
    bucket: String,
    client: aws_sdk_s3::Client,
}

#[async_trait]
impl BackupStorage for S3Storage {
    async fn upload_file(&self, local_path: &Path, remote_path: &str) -> Result<()> {
        let body = ByteStream::from_path(local_path).await?;
        
        self.client
            .put_object()
            .bucket(&self.bucket)
            .key(remote_path)
            .body(body)
            .send()
            .await?;
        
        Ok(())
    }
    
    // å…¶ä»–æ–¹æ³•å®ç°...
}

impl BackupManager {
    pub async fn new(
        db: Arc<DB>,
        storage: Arc<dyn BackupStorage>,
        config: BackupConfig
    ) -> Result<Self> {
        Ok(Self {
            db,
            storage,
            config,
            state: Arc::new(RwLock::new(BackupState::default())),
        })
    }
    
    // å¯åŠ¨åå°å¤‡ä»½ä»»åŠ¡
    pub async fn start(&self) -> Result<()> {
        let manager = self.clone();
        
        tokio::spawn(async move {
            manager.backup_loop().await;
        });
        
        Ok(())
    }
    
    async fn backup_loop(&self) {
        let mut snapshot_timer = tokio::time::interval(self.config.snapshot_interval);
        let mut wal_timer = tokio::time::interval(self.config.wal_archive_interval);
        
        loop {
            tokio::select! {
                _ = snapshot_timer.tick() => {
                    if let Err(e) = self.create_snapshot().await {
                        error!("Snapshot failed: {}", e);
                    }
                }
                _ = wal_timer.tick() => {
                    if let Err(e) = self.archive_wal().await {
                        error!("WAL archive failed: {}", e);
                    }
                }
            }
        }
    }
    
    // åˆ›å»ºå¿«ç…§
    pub async fn create_snapshot(&self) -> Result<SnapshotId> {
        let snapshot_id = SnapshotId::new();
        info!("Creating snapshot {}", snapshot_id);
        
        // 1. è§¦å‘DB checkpoint
        let checkpoint = self.db.create_checkpoint()?;
        
        // 2. ä¸Šä¼ SSTableæ–‡ä»¶
        for sstable_path in &checkpoint.sstables {
            let remote_path = format!("snapshots/{}/{}", 
                                     snapshot_id, 
                                     sstable_path.file_name().unwrap().to_str().unwrap());
            self.storage.upload_file(sstable_path, &remote_path).await?;
        }
        
        // 3. ä¸Šä¼ Manifest
        let manifest_remote = format!("snapshots/{}/MANIFEST", snapshot_id);
        self.storage.upload_file(&checkpoint.manifest, &manifest_remote).await?;
        
        // 4. å†™å…¥å…ƒæ•°æ®
        let metadata = SnapshotMetadata {
            id: snapshot_id.clone(),
            created_at: SystemTime::now(),
            file_count: checkpoint.sstables.len(),
            total_size: checkpoint.total_size,
        };
        self.save_snapshot_metadata(&metadata).await?;
        
        // 5. æ›´æ–°çŠ¶æ€
        self.state.write().last_snapshot = Some(snapshot_id.clone());
        
        info!("Snapshot {} created successfully", snapshot_id);
        Ok(snapshot_id)
    }
    
    // å½’æ¡£WAL
    pub async fn archive_wal(&self) -> Result<()> {
        let last_archived = self.state.read().last_archived_wal;
        
        // 1. è·å–éœ€è¦å½’æ¡£çš„WALæ–‡ä»¶
        let wal_files = self.db.get_wal_files_since(last_archived)?;
        
        if wal_files.is_empty() {
            return Ok(());
        }
        
        info!("Archiving {} WAL files", wal_files.len());
        
        // 2. å¹¶å‘ä¸Šä¼ 
        let mut tasks = Vec::new();
        for wal_file in &wal_files {
            let storage = self.storage.clone();
            let file_path = wal_file.path.clone();
            let remote_path = format!("wal/{}", wal_file.id);
            
            let task = tokio::spawn(async move {
                storage.upload_file(&file_path, &remote_path).await
            });
            
            tasks.push(task);
        }
        
        // 3. ç­‰å¾…å…¨éƒ¨å®Œæˆ
        for task in tasks {
            task.await??;
        }
        
        // 4. æ›´æ–°çŠ¶æ€
        if let Some(last_wal) = wal_files.last() {
            self.state.write().last_archived_wal = Some(last_wal.id);
        }
        
        info!("WAL archiving completed");
        Ok(())
    }
    
    // æ¸…ç†æ—§å¤‡ä»½
    pub async fn cleanup_old_backups(&self) -> Result<()> {
        // æ ¹æ®retention policyæ¸…ç†
        // ...
        Ok(())
    }
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] BackupManagerå®ç°
- [ ] S3/OSSå­˜å‚¨é€‚é…
- [ ] å¿«ç…§åˆ›å»º
- [ ] WALå½’æ¡£
- [ ] ä¿ç•™ç­–ç•¥
- [ ] æµ‹è¯•

---

### Week 37-38: æ¢å¤æœºåˆ¶

**ç›®æ ‡**ï¼šä»å¤‡ä»½æ¢å¤æ•°æ®

```rust
// src/backup/recovery.rs
pub struct RecoveryManager {
    storage: Arc<dyn BackupStorage>,
    target_dir: PathBuf,
}

impl RecoveryManager {
    pub async fn recover_from_snapshot(
        &self,
        snapshot_id: SnapshotId
    ) -> Result<()> {
        info!("Recovering from snapshot {}", snapshot_id);
        
        // 1. ä¸‹è½½snapshotå…ƒæ•°æ®
        let metadata = self.load_snapshot_metadata(&snapshot_id).await?;
        
        // 2. ä¸‹è½½æ‰€æœ‰SSTableæ–‡ä»¶
        let sstable_files = self.storage
            .list_files(&format!("snapshots/{}/", snapshot_id))
            .await?;
        
        info!("Downloading {} files", sstable_files.len());
        
        let mut tasks = Vec::new();
        for remote_file in sstable_files {
            let local_file = self.target_dir.join(
                Path::new(&remote_file).file_name().unwrap()
            );
            let storage = self.storage.clone();
            
            let task = tokio::spawn(async move {
                storage.download_file(&remote_file, &local_file).await
            });
            
            tasks.push(task);
        }
        
        for task in tasks {
            task.await??;
        }
        
        // 3. ä¸‹è½½Manifest
        let manifest_remote = format!("snapshots/{}/MANIFEST", snapshot_id);
        let manifest_local = self.target_dir.join("MANIFEST");
        self.storage.download_file(&manifest_remote, &manifest_local).await?;
        
        info!("Snapshot recovery completed");
        Ok(())
    }
    
    pub async fn replay_wal(
        &self,
        from_wal: Option<WalId>,
        db: &mut DB
    ) -> Result<()> {
        info!("Replaying WAL from {:?}", from_wal);
        
        // 1. åˆ—å‡ºéœ€è¦replayçš„WALæ–‡ä»¶
        let wal_files = if let Some(from) = from_wal {
            self.storage.list_files(&format!("wal/{}*", from)).await?
        } else {
            self.storage.list_files("wal/").await?
        };
        
        info!("Found {} WAL files to replay", wal_files.len());
        
        // 2. æŒ‰é¡ºåºä¸‹è½½å’Œreplay
        for wal_file in wal_files {
            let local_file = self.target_dir.join("temp_wal");
            self.storage.download_file(&wal_file, &local_file).await?;
            
            // Replayåˆ°DB
            db.replay_wal(&local_file)?;
            
            // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            fs::remove_file(&local_file)?;
        }
        
        info!("WAL replay completed");
        Ok(())
    }
    
    // å®Œæ•´æ¢å¤æµç¨‹
    pub async fn full_recovery(&self, snapshot_id: Option<SnapshotId>) -> Result<DB> {
        // 1. æ¢å¤å¿«ç…§ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        let snapshot_id = if let Some(id) = snapshot_id {
            id
        } else {
            // æ‰¾åˆ°æœ€æ–°çš„å¿«ç…§
            self.find_latest_snapshot().await?
        };
        
        self.recover_from_snapshot(snapshot_id.clone()).await?;
        
        // 2. æ‰“å¼€DB
        let mut db = DB::open(&self.target_dir, Options::default())?;
        
        // 3. Replayå¿«ç…§ä¹‹åçš„WAL
        self.replay_wal(Some(snapshot_id.wal_position), &mut db).await?;
        
        Ok(db)
    }
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] RecoveryManagerå®ç°
- [ ] å¿«ç…§ä¸‹è½½
- [ ] WAL replay
- [ ] å®Œæ•´æ¢å¤æµç¨‹
- [ ] æ¢å¤æµ‹è¯•
- [ ] ç¾éš¾æ¢å¤æ¼”ç»ƒ

---

### Week 39-40: å¤‡ä»½æ¢å¤é›†æˆæµ‹è¯•

**ç›®æ ‡**ï¼šéªŒè¯ç«¯åˆ°ç«¯å¤‡ä»½æ¢å¤

```rust
#[tokio::test]
async fn test_backup_and_recovery() -> Result<()> {
    // 1. åˆ›å»ºDBå¹¶å†™å…¥æ•°æ®
    let primary_config = PrimaryConfig {
        db_path: "/tmp/test_primary".into(),
        ..Default::default()
    };
    let mut primary = PrimaryNode::new(primary_config).await?;
    primary.start().await?;
    
    // å†™å…¥æµ‹è¯•æ•°æ®
    for i in 0..10000 {
        let key = format!("key{}", i).into_bytes();
        let value = format!("value{}", i).into_bytes();
        primary.put(&key, &value).await?;
    }
    
    // 2. åˆ›å»ºå¤‡ä»½
    let storage = Arc::new(S3Storage::new("test-bucket"));
    let backup_config = BackupConfig {
        snapshot_interval: Duration::from_secs(60),
        ..Default::default()
    };
    let backup_mgr = BackupManager::new(
        primary.db.clone(),
        storage.clone(),
        backup_config
    ).await?;
    
    let snapshot_id = backup_mgr.create_snapshot().await?;
    
    // 3. åœæ­¢åŸDB
    primary.stop().await?;
    fs::remove_dir_all("/tmp/test_primary")?;
    
    // 4. ä»å¤‡ä»½æ¢å¤
    let recovery_mgr = RecoveryManager {
        storage,
        target_dir: "/tmp/test_recovery".into(),
    };
    
    let recovered_db = recovery_mgr.full_recovery(Some(snapshot_id)).await?;
    
    // 5. éªŒè¯æ•°æ®
    for i in 0..10000 {
        let key = format!("key{}", i).into_bytes();
        let value = recovered_db.get(&key)?;
        assert_eq!(value, Some(format!("value{}", i).into_bytes()));
    }
    
    Ok(())
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] æ•…éšœæ³¨å…¥æµ‹è¯•
- [ ] å¤§æ•°æ®é‡æµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•ï¼ˆå¤‡ä»½å’Œæ¢å¤é€Ÿåº¦ï¼‰
- [ ] æ–‡æ¡£

**é˜¶æ®µ4äº¤ä»˜ç‰©**ï¼š
- âœ… å¼‚æ­¥å¤‡ä»½åˆ°ç½‘ç›˜
- âœ… ä»å¤‡ä»½æ¢å¤
- âœ… å®Œæ•´çš„ç¾éš¾æ¢å¤æ–¹æ¡ˆ
- âœ… æµ‹è¯•éªŒè¯

---

## ğŸ“‹ é˜¶æ®µ5: å¼¹æ€§ä¼¸ç¼©ï¼ˆWeek 41-44ï¼‰

### Week 41-42: åŠ¨æ€æ‰©å±•å®ç°

**ç›®æ ‡**ï¼šæ”¯æŒåœ¨çº¿æ·»åŠ /ç§»é™¤èŠ‚ç‚¹

```rust
// src/cluster/scaling.rs
pub struct ScalingManager {
    coordinator: Arc<Coordinator>,
}

impl ScalingManager {
    // æ·»åŠ æ–°Shard
    pub async fn add_shard(
        &self,
        primary_addr: String,
        data_dir: PathBuf
    ) -> Result<ShardId> {
        let shard_id = ShardId::new_random();
        
        info!("Adding new shard {}", shard_id);
        
        // 1. å¯åŠ¨æ–°çš„Primary
        let primary_config = PrimaryConfig {
            listen_addr: primary_addr.parse()?,
            db_path: data_dir,
            ..Default::default()
        };
        let mut primary = PrimaryNode::new(primary_config).await?;
        primary.start().await?;
        
        // 2. åˆ›å»ºShard Group
        let shard_group = ShardGroup {
            shard_id,
            primary: PrimaryInfo {
                addr: primary_addr.clone(),
                client: Arc::new(Mutex::new(
                    StorageClient::connect(&primary_addr).await?
                )),
            },
            replicas: vec![],
            health: ShardHealth::Healthy,
        };
        
        // 3. æ³¨å†Œåˆ°Coordinator
        self.coordinator.register_shard(shard_group).await?;
        
        info!("Shard {} added successfully", shard_id);
        Ok(shard_id)
    }
    
    // æ·»åŠ Replicaåˆ°ç°æœ‰Shard
    pub async fn add_replica(
        &self,
        shard_id: ShardId,
        replica_addr: String
    ) -> Result<ReplicaId> {
        let replica_id = ReplicaId::new_random();
        
        info!("Adding replica {} to shard {}", replica_id, shard_id);
        
        // 1. è·å–Primaryåœ°å€
        let primary_addr = {
            let shard_groups = self.coordinator.shard_groups.read();
            let shard = shard_groups.get(&shard_id)
                .ok_or_else(|| Error::not_found("Shard not found"))?;
            shard.primary.addr.clone()
        };
        
        // 2. åˆ›å»ºReplica
        let replica_config = ReplicaConfig {
            primary_addr,
            cache_size: 1024 * 1024 * 1024, // 1GB
            warmup_strategy: WarmupStrategy::None, // æ‡’åŠ è½½
            ..Default::default()
        };
        let replica = ReplicaNode::new(replica_config).await?;
        
        // 3. æ·»åŠ åˆ°Shard Group
        let mut shard_groups = self.coordinator.shard_groups.write();
        let shard = shard_groups.get_mut(&shard_id)
            .ok_or_else(|| Error::not_found("Shard not found"))?;
        
        shard.replicas.push(ReplicaInfo {
            id: replica_id,
            addr: replica_addr,
            client: Arc::new(Mutex::new(
                StorageClient::connect(&shard.primary.addr).await?
            )),
            load: AtomicU64::new(0),
        });
        
        info!("Replica {} added to shard {}", replica_id, shard_id);
        Ok(replica_id)
    }
    
    // ç§»é™¤Replica
    pub async fn remove_replica(
        &self,
        shard_id: ShardId,
        replica_id: ReplicaId
    ) -> Result<()> {
        info!("Removing replica {} from shard {}", replica_id, shard_id);
        
        let mut shard_groups = self.coordinator.shard_groups.write();
        let shard = shard_groups.get_mut(&shard_id)
            .ok_or_else(|| Error::not_found("Shard not found"))?;
        
        shard.replicas.retain(|r| r.id != replica_id);
        
        info!("Replica {} removed from shard {}", replica_id, shard_id);
        Ok(())
    }
    
    // ç§»é™¤Shardï¼ˆéœ€è¦è°¨æ…æ“ä½œï¼‰
    pub async fn remove_shard(&self, shard_id: ShardId) -> Result<()> {
        warn!("Removing shard {} - this will make its data inaccessible", shard_id);
        
        // 1. ä»hash ringç§»é™¤ï¼ˆåœæ­¢æ–°è¯·æ±‚ï¼‰
        self.coordinator.hash_ring.write().remove_node(shard_id);
        
        // 2. ç­‰å¾…ç°æœ‰è¯·æ±‚å®Œæˆ
        tokio::time::sleep(Duration::from_secs(5)).await;
        
        // 3. åˆ›å»ºæœ€ç»ˆå¤‡ä»½
        let shard_groups = self.coordinator.shard_groups.read();
        let shard = shard_groups.get(&shard_id)
            .ok_or_else(|| Error::not_found("Shard not found"))?;
        
        // TODO: è§¦å‘å¤‡ä»½
        
        // 4. ä»Coordinatorç§»é™¤
        drop(shard_groups);
        self.coordinator.shard_groups.write().remove(&shard_id);
        
        warn!("Shard {} removed", shard_id);
        Ok(())
    }
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] æ·»åŠ Shardå®ç°
- [ ] æ·»åŠ Replicaå®ç°
- [ ] ç§»é™¤èŠ‚ç‚¹å®ç°
- [ ] å®‰å…¨æ£€æŸ¥
- [ ] æµ‹è¯•

---

### Week 43-44: è‡ªåŠ¨ä¼¸ç¼©ï¼ˆå¯é€‰ï¼‰

**ç›®æ ‡**ï¼šåŸºäºè´Ÿè½½è‡ªåŠ¨æ‰©ç¼©å®¹

```rust
// src/cluster/autoscaler.rs
pub struct AutoScaler {
    coordinator: Arc<Coordinator>,
    scaling_mgr: Arc<ScalingManager>,
    config: AutoScalerConfig,
}

pub struct AutoScalerConfig {
    // Replicaä¼¸ç¼©é˜ˆå€¼
    replica_scale_up_threshold: f64,    // CPU > 80%
    replica_scale_down_threshold: f64,  // CPU < 20%
    
    // Shardä¼¸ç¼©é˜ˆå€¼
    shard_scale_up_threshold: f64,      // æ‰€æœ‰shardè´Ÿè½½é«˜
    
    // æ£€æŸ¥é—´éš”
    check_interval: Duration,
    
    // å†·å´æ—¶é—´
    cooldown: Duration,
}

impl AutoScaler {
    pub async fn start(&self) {
        let mut interval = tokio::time::interval(self.config.check_interval);
        
        loop {
            interval.tick().await;
            
            if let Err(e) = self.evaluate_and_scale().await {
                error!("Auto-scaling error: {}", e);
            }
        }
    }
    
    async fn evaluate_and_scale(&self) -> Result<()> {
        // 1. æ”¶é›†æ‰€æœ‰shardçš„æŒ‡æ ‡
        let metrics = self.collect_metrics().await?;
        
        // 2. è¯„ä¼°æ˜¯å¦éœ€è¦æ‰©å±•Replica
        for (shard_id, shard_metrics) in &metrics.shards {
            if shard_metrics.avg_cpu > self.config.replica_scale_up_threshold {
                info!("Shard {} is overloaded, adding replica", shard_id);
                // æ·»åŠ æ–°replica
                self.scaling_mgr.add_replica(*shard_id, "...".to_string()).await?;
            } else if shard_metrics.avg_cpu < self.config.replica_scale_down_threshold
                && shard_metrics.replica_count > 1 {
                info!("Shard {} is underloaded, removing replica", shard_id);
                // ç§»é™¤ä¸€ä¸ªreplica
                // ...
            }
        }
        
        // 3. è¯„ä¼°æ˜¯å¦éœ€è¦æ·»åŠ Shard
        if metrics.overall_load > self.config.shard_scale_up_threshold {
            info!("Overall load is high, adding new shard");
            self.scaling_mgr.add_shard("...".to_string(), PathBuf::new()).await?;
        }
        
        Ok(())
    }
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] æŒ‡æ ‡æ”¶é›†
- [ ] ä¼¸ç¼©ç­–ç•¥å®ç°
- [ ] å†·å´æ—¶é—´æ§åˆ¶
- [ ] æµ‹è¯•
- [ ] æ–‡æ¡£

**é˜¶æ®µ5äº¤ä»˜ç‰©**ï¼š
- âœ… æ‰‹åŠ¨æ·»åŠ /ç§»é™¤èŠ‚ç‚¹
- âœ… è‡ªåŠ¨ä¼¸ç¼©ï¼ˆå¯é€‰ï¼‰
- âœ… æµ‹è¯•éªŒè¯
- âœ… è¿ç»´æ–‡æ¡£

---

## ğŸ“‹ é˜¶æ®µ6: ç›‘æ§å’Œè¿ç»´ï¼ˆWeek 45-48ï¼‰

### Week 45-46: ç›‘æ§æŒ‡æ ‡

**ç›®æ ‡**ï¼šå®Œæ•´çš„å¯è§‚æµ‹æ€§

```rust
// src/metrics/mod.rs
use prometheus::{Registry, Counter, Histogram, Gauge};

pub struct Metrics {
    registry: Registry,
    
    // è¯·æ±‚æŒ‡æ ‡
    pub requests_total: Counter,
    pub request_duration: Histogram,
    
    // æ•°æ®æŒ‡æ ‡
    pub keys_total: Gauge,
    pub data_size_bytes: Gauge,
    
    // ShardæŒ‡æ ‡
    pub shards_total: Gauge,
    pub replicas_per_shard: Histogram,
    
    // ç¼“å­˜æŒ‡æ ‡
    pub cache_hits_total: Counter,
    pub cache_misses_total: Counter,
    pub cache_size_bytes: Gauge,
    
    // å¤‡ä»½æŒ‡æ ‡
    pub backup_last_success_timestamp: Gauge,
    pub backup_duration_seconds: Histogram,
}

impl Metrics {
    pub fn new() -> Result<Self> {
        let registry = Registry::new();
        
        // æ³¨å†Œæ‰€æœ‰æŒ‡æ ‡...
        
        Ok(Self {
            registry,
            // ...
        })
    }
    
    pub fn export(&self) -> String {
        use prometheus::Encoder;
        let encoder = prometheus::TextEncoder::new();
        
        let mut buffer = vec![];
        encoder.encode(&self.registry.gather(), &mut buffer).unwrap();
        
        String::from_utf8(buffer).unwrap()
    }
}

// HTTPæœåŠ¡å¯¼å‡ºæŒ‡æ ‡
pub async fn metrics_server(metrics: Arc<Metrics>, addr: SocketAddr) {
    let app = Router::new()
        .route("/metrics", get(|| async move {
            metrics.export()
        }));
    
    axum::Server::bind(&addr)
        .serve(app.into_make_service())
        .await
        .unwrap();
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] PrometheusæŒ‡æ ‡å®šä¹‰
- [ ] æŒ‡æ ‡æ”¶é›†ç‚¹åŸ‹ç‚¹
- [ ] HTTP metrics endpoint
- [ ] Grafana dashboard
- [ ] å‘Šè­¦è§„åˆ™
- [ ] æ–‡æ¡£

---

### Week 47-48: è¿ç»´å·¥å…·

**ç›®æ ‡**ï¼šæ–¹ä¾¿çš„è¿ç»´å‘½ä»¤

```rust
// src/bin/aidb-admin.rs
use clap::{Parser, Subcommand};

#[derive(Parser)]
#[command(name = "aidb-admin")]
#[command(about = "AiDb cluster administration tool")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// List all shards
    ListShards {
        #[arg(short, long)]
        coordinator_addr: String,
    },
    
    /// Add a new shard
    AddShard {
        #[arg(long)]
        coordinator_addr: String,
        #[arg(long)]
        primary_addr: String,
        #[arg(long)]
        data_dir: String,
    },
    
    /// Add a replica to a shard
    AddReplica {
        #[arg(long)]
        coordinator_addr: String,
        #[arg(long)]
        shard_id: String,
        #[arg(long)]
        replica_addr: String,
    },
    
    /// Remove a replica
    RemoveReplica {
        #[arg(long)]
        coordinator_addr: String,
        #[arg(long)]
        shard_id: String,
        #[arg(long)]
        replica_id: String,
    },
    
    /// Show cluster status
    Status {
        #[arg(short, long)]
        coordinator_addr: String,
    },
    
    /// Create a backup
    Backup {
        #[arg(long)]
        shard_id: String,
    },
    
    /// Recover from backup
    Recover {
        #[arg(long)]
        snapshot_id: String,
        #[arg(long)]
        target_dir: String,
    },
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();
    
    match cli.command {
        Commands::ListShards { coordinator_addr } => {
            // è¿æ¥coordinatorå¹¶åˆ—å‡ºæ‰€æœ‰shard
            let client = AdminClient::connect(&coordinator_addr).await?;
            let shards = client.list_shards().await?;
            
            for shard in shards {
                println!("Shard {}: {} replicas, status: {:?}", 
                        shard.id, shard.replica_count, shard.status);
            }
        }
        
        Commands::AddShard { coordinator_addr, primary_addr, data_dir } => {
            let client = AdminClient::connect(&coordinator_addr).await?;
            let shard_id = client.add_shard(primary_addr, data_dir).await?;
            
            println!("Shard {} added successfully", shard_id);
        }
        
        Commands::Status { coordinator_addr } => {
            let client = AdminClient::connect(&coordinator_addr).await?;
            let status = client.get_status().await?;
            
            println!("Cluster Status:");
            println!("  Shards: {}", status.shard_count);
            println!("  Total replicas: {}", status.replica_count);
            println!("  Healthy shards: {}", status.healthy_shards);
            println!("  Total keys: {}", status.total_keys);
            println!("  Total size: {} GB", status.total_size_gb);
        }
        
        // å…¶ä»–å‘½ä»¤...
        _ => {}
    }
    
    Ok(())
}
```

**ä»»åŠ¡æ¸…å•**ï¼š
- [ ] å‘½ä»¤è¡Œå·¥å…·å®ç°
- [ ] è¿ç»´è„šæœ¬
- [ ] éƒ¨ç½²æ–‡æ¡£
- [ ] æ•…éšœæ’æŸ¥æŒ‡å—
- [ ] æœ€ä½³å®è·µæ–‡æ¡£

**é˜¶æ®µ6äº¤ä»˜ç‰©**ï¼š
- âœ… å®Œæ•´çš„ç›‘æ§ç³»ç»Ÿ
- âœ… è¿ç»´å·¥å…·
- âœ… æ–‡æ¡£å®Œå–„
- âœ… ç”Ÿäº§å°±ç»ª

---

## ğŸ“Š æ€»ä½“æ—¶é—´è¡¨

```
Week 1-20:  é˜¶æ®µ0 - å•æœºç‰ˆLSMå¼•æ“ âœ… å·²è§„åˆ’
Week 21-24: é˜¶æ®µ1 - RPCå’Œç½‘ç»œå±‚
Week 25-28: é˜¶æ®µ2 - Coordinator
Week 29-34: é˜¶æ®µ3 - Shard Group
Week 35-40: é˜¶æ®µ4 - Backupå’Œæ¢å¤
Week 41-44: é˜¶æ®µ5 - å¼¹æ€§ä¼¸ç¼©
Week 45-48: é˜¶æ®µ6 - ç›‘æ§å’Œè¿ç»´

æ€»è®¡: 48å‘¨ (çº¦12ä¸ªæœˆ)
```

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

### åŠŸèƒ½å®Œæ•´æ€§
- âœ… å¤šShardåˆ†ç‰‡å­˜å‚¨
- âœ… Primary + Replicasæ¶æ„
- âœ… è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
- âœ… å¼‚æ­¥å¤‡ä»½å’Œæ¢å¤
- âœ… å¼¹æ€§ä¼¸ç¼©

### æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ |
|------|------|
| å•shardå†™å…¥ | 50K ops/s |
| 10 shardså†™å…¥ | 500K ops/s |
| Replicaç¼“å­˜å‘½ä¸­è¯» | 500K ops/s |
| Replicaè½¬å‘è¯» | 30K ops/s |
| P99å»¶è¿Ÿ(å†™) | < 5ms |
| P99å»¶è¿Ÿ(ç¼“å­˜å‘½ä¸­è¯») | < 1ms |

### å¯ç”¨æ€§ç›®æ ‡
- å•shardæ¢å¤æ—¶é—´ï¼š< 5åˆ†é’Ÿ
- Replicaæ•…éšœå½±å“ï¼šæ— ï¼ˆè‡ªåŠ¨åˆ‡æ¢ï¼‰
- æ•°æ®ä¸¢å¤±çª—å£ï¼š< 10åˆ†é’Ÿï¼ˆWALå½’æ¡£é¢‘ç‡ï¼‰

### æˆæœ¬ç›®æ ‡
- ç›¸æ¯”å…¨é‡å¤åˆ¶ï¼šå­˜å‚¨æˆæœ¬é™ä½50-60%
- ç½‘ç»œæˆæœ¬é™ä½80%+

---

## ğŸ“š å‚è€ƒæ–‡æ¡£

### å·²æœ‰æ–‡æ¡£
- `OPTIMIZED_PLAN.md` - å•æœºç‰ˆå®æ–½è®¡åˆ’
- `ROCKSDB_LESSONS.md` - RocksDBç»éªŒå€Ÿé‰´
- `SCALABLE_CLUSTER_DESIGN.md` - é›†ç¾¤æ¶æ„è®¾è®¡
- `SHARED_STORAGE_REEVALUATION.md` - æ¶æ„è¯„ä¼°

### å¾…å®Œæˆæ–‡æ¡£
- APIæ–‡æ¡£
- è¿ç»´æ‰‹å†Œ
- æ•…éšœæ’æŸ¥æŒ‡å—
- æ€§èƒ½è°ƒä¼˜æŒ‡å—

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç¡®è®¤è®¡åˆ’**ï¼šè¿™ä¸ªå®æ–½è®¡åˆ’æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Ÿ
2. **èµ„æºåˆ†é…**ï¼šéœ€è¦å‡ ä¸ªå¼€å‘äººå‘˜ï¼Ÿ
3. **å¼€å§‹å®æ–½**ï¼šä»é˜¶æ®µ0ï¼ˆå•æœºç‰ˆï¼‰å¼€å§‹

---

*å®Œæ•´çš„å¼¹æ€§é›†ç¾¤å®æ–½è®¡åˆ’åˆ¶å®šå®Œæˆï¼*

*é¢„è®¡12ä¸ªæœˆå®Œæˆå•æœºç‰ˆ + å®Œæ•´é›†ç¾¤åŠŸèƒ½*
